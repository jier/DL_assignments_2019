[DEBUG] Training start...
 Accuracy 0.15019999622056882 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 100 --optimizer SGD
 Accuracy 0.35929999166727067 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 100 --optimizer Adam
 Accuracy 0.43389998869101204 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 100 --optimizer Adamax
 Accuracy 0.4382999886075656 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 100 --optimizer Adagrad
 Accuracy 0.15289999630053838 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 200 --optimizer SGD
 Accuracy 0.4391999895175298 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 200 --optimizer Adam
 Accuracy 0.4625999889175097 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 200 --optimizer Adamax
 Accuracy 0.4470999893347422 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 200 --optimizer Adagrad
 Accuracy 0.15783200696110725 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 250 --optimizer SGD
 Accuracy 0.45212268781661985 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 250 --optimizer Adam
 Accuracy 0.44733335453271866 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 250 --optimizer Adamax
 Accuracy 0.4489973543683688 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 1500 --batch_size 250 --optimizer Adagrad
 Accuracy 0.15019999623298644 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 3000 --batch_size 100 --optimizer SGD
 Accuracy 0.3820999906708797 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 3000 --batch_size 100 --optimizer Adam
 Accuracy 0.439999988541007 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 3000 --batch_size 100 --optimizer Adamax
 Accuracy 0.46169998780886334 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 3000 --batch_size 100 --optimizer Adagrad
 Accuracy 0.15289999625707665 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 3000 --batch_size 200 --optimizer SGD
 Accuracy 0.42419998968640965 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 3000 --batch_size 200 --optimizer Adam
 Accuracy 0.4827999885181586 --dnn_hidden_units 100, 100 --learning_rate 0.02 --max_steps 3000 --batch_size 200 --optimizer Adamax
